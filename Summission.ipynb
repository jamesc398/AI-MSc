{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesc398/AI-MSc/blob/main/Summission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6go6O1NG6Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2003b9a-4d48-497f-c046-c04fe7731552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# importing neccesary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.svm import SVC\n",
        "import nltk\n",
        "import re\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6I61cq4Imi5"
      },
      "source": [
        "The above code snippet imports the neccesesary libraries required for this text classification. Panda's will be utillsed for data manipulation. Sklearn will be used for its machine learning applications. NLTK has been selected for its NLP processing. The code snippet also imports the required modules from vectorization, hyperparameter tuning along with metrics for evaluation.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "be0kz_OL5EyV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAEGukknQx6c",
        "outputId": "3b8a281c-3adb-4455-979a-0e442a929350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# To ensure NLTK data is availble\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f86kE5PpWvxI"
      },
      "outputs": [],
      "source": [
        "# File paths to datasets and output\n",
        "train_images_path = '/content/drive/My Drive/data (3/New folder/training_images/'\n",
        "train_images_path = '/content/drive/My Drive/data (3)/New folder/test_images/'\n",
        "test_csv_path = '/content/drive/My Drive/data (3)/New folder/train.csv'\n",
        "test_csv_path = '/content/drive/My Drive/data (3)/New folder/test.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro-87kMwf7Mx"
      },
      "outputs": [],
      "source": [
        "# Preprocessing steps\n",
        "def preprocess_text(text):\n",
        "  if pd.isnull(text):\n",
        "    return \"\"\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'\\@\\w+|\\#' , '', text)\n",
        "  text_tokens = text.split()\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  tokens_without_sw =[word for word in text_tokens if word.lower() not in stopwords]\n",
        "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  lemma_words = [lemmatizer.lemmatize(w) for w in tokens_without_sw]\n",
        "  return \"\".join(lemma_words)\n",
        "\n",
        "# applying preprocessing\n",
        "train_df['cleaned_text'] = train_df['tweet'].apply(preprocess_text)\n",
        "test_df['cleaned_text'] = test_df['tweet'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiXNncN_f6r-"
      },
      "source": [
        "The processing steps listed above cleans the data by removing url's, mention and hashtags. It then proceeds to token the text removing stopwords, and lemmatizes words to base forms while maintaining the semantic meaning of the text. This in turn enhances text analysis for modelling. By cleaning the data effectively this should improve the overall accurracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey8KY_7akRG4"
      },
      "outputs": [],
      "source": [
        "'])# Feature extraction steps\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "y_train = train_df['label']\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature extraction technique used is TF-IDF vectorization to convert cleaned text into a numerical format that the model can process for training and testing."
      ],
      "metadata": {
        "id": "LxC0EQWA79wY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeNEeyhAq4Me"
      },
      "outputs": [],
      "source": [
        "# Split tha data\n",
        "X_train, X_val, y_train_test_split(\n",
        "    x_train_tfidf, y_train, test_size=0.3, random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3Fgzv-XrqIV"
      },
      "outputs": [],
      "source": [
        "# Model training\n",
        "parameters ={'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}\n",
        "svc= SVC(\n",
        "clf = GridsearchCV(svc, parameters, cv=5, scoring='f1_weighted')\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYVnzVAXtFkf"
      },
      "outputs": [],
      "source": [
        "# Select best model\n",
        "best_model = clf.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Js-1IhTtbs5"
      },
      "outputs": [],
      "source": [
        "# Validate the model\n",
        "val_predictions = best_model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, val-predictions)\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support9y_val, val_predictions, average='weighted')\n",
        "print(f\"SVM Best Model - Accuracy: {accurary}, Precision: {prescision}, Recall: {recall), F1 Score: {fscore}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo9XGswGvO3w"
      },
      "outputs": [],
      "source": [
        "#Predict on the test set\n",
        "predictions = best_model.predict(X_test_tfidf)\n",
        "test_df['predictions'] = predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VmOumCQvmNJ"
      },
      "outputs": [],
      "source": [
        "# Save Predictions to JSON file\n",
        "test_df[['id', 'prediction']].to_json(output_file_path, orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code outlines the process of model training for a text classification task, by the using the SVM model optimised through hypoparameters tunning. The gridsearchCV method from Sklearn.scikt_learn is used to explore a range of values for parameters such as 'c'(regularsation strength), 'gamma', (kernel coefficeint), and 'kernel' type, aiming to find the right combination that yeilds the best performance in terms of F1 score, a balanced measure of precision and recall. This search validailty across 5 folds of training data, ensures the model is neither underfitting or overfitting which enhances its generalisability to unseen data.\n",
        "\n",
        "once the best model is selected it is then validaity on a seperate set to assess its accurracy, F1 score. These metrics provide a view highlighting its strengths and areas the need further development.\n",
        "\n",
        "The F1 score enables precision and recall, offering a single metric to evaluate the model, espically in terms of in balanced datasets.\n",
        "\n",
        "The final step involved applying the best model to the test data set transfoming the textual data into TF-IDF features consist with the training phase. Predictions are then appended to the test dataset, which shows the models ability to classify, new unseen text data.\n",
        "\n",
        "The output is then saved to a JSON file for easy sharing and analysis."
      ],
      "metadata": {
        "id": "bL8xQP939xn9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "11toaYmKcDJ0DydcpogdxiJlHd1bxh0fR",
      "authorship_tag": "ABX9TyMzrWKJaa54FbuGksFW+lLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}